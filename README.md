# üõí E-Commerce Customer Analytics

Pipeline de an√°lisis end-to-end sobre datos de clientes de e-commerce: desde la normalizaci√≥n de CSVs crudos hasta un modelo predictivo de churn con segmentaci√≥n de compradores.

---

## üìã Descripci√≥n del proyecto

Este proyecto construye un pipeline de datos completo que cubre las etapas fundamentales de un proyecto de analytics real:

1. **Modelado de base de datos** en esquema copo de nieve (Snowflake Schema)
2. **ETL** desde CSVs crudos de Kaggle hacia SQLite
3. **An√°lisis exploratorio** (EDA) con estad√≠sticas descriptivas y correlaciones
4. **Segmentaci√≥n de clientes** con K-Means (Buyer Personas)
5. **Modelo predictivo de churn** con Logistic Regression y XGBoost
6. **An√°lisis de cierre** cruzando segmentos con scores de riesgo

> **Nota sobre los datos:** Los datasets son sint√©ticos obtenidos de Kaggle. Los scores del modelo son excepcionalmente altos (AUC ~0.99) precisamente porque las variables fueron construidas para predecir el churn ‚Äî algo esperado y documentado en el an√°lisis.

---

## üóÇÔ∏è Estructura del repositorio

```
ecommerce_db/
‚îÇ
‚îú‚îÄ‚îÄ config.py                  # Rutas y constantes centralizadas
‚îÇ
‚îú‚îÄ‚îÄ 01_create_schema.py        # DDL: crea tablas, √≠ndices y vistas en SQLite
‚îú‚îÄ‚îÄ 02_load_customers.py       # ETL: carga clientes (features + targets)
‚îú‚îÄ‚îÄ 03_load_orders.py          # ETL: carga √≥rdenes y dimensiones de fecha
‚îú‚îÄ‚îÄ 04_validate.py             # Validaci√≥n de integridad referencial
‚îÇ
‚îú‚îÄ‚îÄ 05_eda.py                  # An√°lisis exploratorio (4 secciones)
‚îú‚îÄ‚îÄ 06_segmentation.py         # K-Means: segmentaci√≥n de clientes
‚îú‚îÄ‚îÄ 07_churn_model.py          # Modelos predictivos de churn
‚îú‚îÄ‚îÄ 08_risk_analysis.py        # An√°lisis de cierre: segmentos √ó riesgo
‚îÇ
‚îú‚îÄ‚îÄ run_all.py                 # Orquestador: corre el pipeline completo
‚îÇ
‚îî‚îÄ‚îÄ snowflake_schema_ddl.sql   # DDL standalone para referencia
```

---

## üóÑÔ∏è Esquema de base de datos

El proyecto implementa un **Snowflake Schema** con dos subject areas independientes:

```
dim_loyalty_status ‚îÄ‚îÄ‚ñ∫ fact_customers
                           (6,000 clientes ¬∑ churn label ¬∑ m√©tricas de comportamiento)

dim_date ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
dim_product_category‚îÄ‚î§
dim_payment_method ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚ñ∫ fact_orders
dim_order_status ‚îÄ‚îÄ‚îÄ‚îÄ‚îò        (2,600 √≥rdenes ¬∑ valor ¬∑ rating ¬∑ estado)

‚îÄ‚îÄ Tablas generadas por el pipeline ‚îÄ‚îÄ
customer_segments   (K-Means: segmento asignado por cliente)
churn_scores        (score 0‚Äì1 ¬∑ tier Bajo/Medio/Alto/Cr√≠tico)
```

Los dos bloques no comparten llaves ‚Äî limitaci√≥n conocida del dataset fuente, documentada en el an√°lisis.

---

## üìä Resultados principales

### Segmentaci√≥n de clientes (K-Means, k=4)

| Segmento | Clientes | Churn rate | Perfil |
|---|---|---|---|
| üíé Cliente Premium | 1,845 (30.8%) | 0.2% | Alto valor, alta frecuencia, muy comprometido |
| üè∑Ô∏è Cazador de Ofertas | 1,928 (32.1%) | 1.1% | Compra por descuento, baja lealtad intr√≠nseca |
| üåü Cliente Leal | 1,362 (22.7%) | 14.4% | Buen engagement, en zona intermedia de riesgo |
| ‚ö†Ô∏è Cliente en Riesgo | 865 (14.4%) | 82.0% | Alta inactividad, pr√°cticamente perdidos |

### Modelo predictivo de churn

| Modelo | Accuracy | Recall | F1 | ROC-AUC |
|---|---|---|---|---|
| Logistic Regression | 0.9608 | **0.9247** | 0.8798 | **0.9931** |
| XGBoost | 0.9592 | 0.9032 | 0.8727 | 0.9925 |

**Ganador: Logistic Regression** ‚Äî mejor recall y AUC. En churn prediction el recall es la m√©trica cr√≠tica: minimizar los falsos negativos (clientes que se van sin ser detectados).

**Variables m√°s importantes:** `days_since_last_purchase` (+305% diferencia entre grupos) y `engagement_score` (5.35 activos vs 2.36 churned).

### An√°lisis de cierre: Cr√≠ticos por segmento

| Segmento | Cr√≠ticos | % del segmento | Acci√≥n recomendada |
|---|---|---|---|
| ‚ö†Ô∏è Cliente en Riesgo | 703 | 81.3% | Ya perdidos ‚Äî bajo ROI de intervenci√≥n |
| üåü Cliente Leal | 179 | 13.1% | **Prioridad de retenci√≥n** |
| üè∑Ô∏è Cazador de Ofertas | 11 | 0.6% | Reactivar con oferta puntual |
| üíé Cliente Premium | 1 | 0.1% | Sin acci√≥n necesaria |

> Los **179 Clientes Leales en riesgo cr√≠tico** son el target real de retenci√≥n ‚Äî tienen historial de valor y todav√≠a son recuperables.

---

## ‚öôÔ∏è Instalaci√≥n y uso

### Requisitos

- Python 3.10+
- Las librer√≠as del proyecto (ver abajo)

### 1. Clonar el repositorio

```bash
git clone https://github.com/tu-usuario/ecommerce-customer-analytics.git
cd ecommerce-customer-analytics
```

### 2. Crear entorno virtual e instalar dependencias

```bash
python -m venv .venv

# Windows
.venv\Scripts\activate

# Mac/Linux
source .venv/bin/activate

pip install -r requirements.txt
```

### 3. Configurar rutas de los CSVs

Edita `config.py` y ajusta las rutas a los archivos fuente:

```python
CSV_FEATURES = Path("ruta/a/ecommerce_customer_features.csv")
CSV_TARGETS  = Path("ruta/a/ecommerce_customer_targets.csv")
CSV_ORDERS   = Path("ruta/a/daily_ecommerce_orders.csv")
```

> ‚ö†Ô∏è En Windows usa barras hacia adelante (`/`) o raw strings (`r"C:\ruta\..."`) para evitar errores de encoding.

### 4. Correr el pipeline

**Opci√≥n A ‚Äî Script por script (recomendado la primera vez):**

```bash
python 01_create_schema.py   # Crea el esquema en SQLite
python 02_load_customers.py  # Carga clientes
python 03_load_orders.py     # Carga √≥rdenes
python 04_validate.py        # Valida integridad

python 05_eda.py             # An√°lisis exploratorio
python 06_segmentation.py    # Segmentaci√≥n K-Means
python 07_churn_model.py     # Modelo de churn
python 08_risk_analysis.py   # An√°lisis de cierre
```

**Opci√≥n B ‚Äî Pipeline completo de una vez:**

```bash
python run_all.py
```

---

## üì¶ Dependencias

```
pandas
matplotlib
seaborn
scikit-learn
xgboost
imbalanced-learn
```

Instala todo con:

```bash
pip install pandas matplotlib seaborn scikit-learn xgboost imbalanced-learn
```

---

## üîç Decisiones t√©cnicas destacadas

**¬øPor qu√© Snowflake Schema y no Star Schema?**
Las dimensiones categ√≥ricas de √≥rdenes (categor√≠a, m√©todo de pago, estado) tienen pocos valores √∫nicos pero se normalizaron igual para demostrar el patr√≥n completo. En producci√≥n con dimensiones m√°s grandes la diferencia de performance ser√≠a significativa.

**¬øPor qu√© SMOTE?**
El dataset tiene desbalance de clases (84.5% activos vs 15.5% churned). Sin balanceo, un modelo que prediga siempre "activo" alcanza 84.5% de accuracy trivialmente. SMOTE genera ejemplos sint√©ticos de la clase minoritaria solo en el set de entrenamiento.

**¬øPor qu√© k=4 si el silhouette score √≥ptimo es k=2?**
Los datos sint√©ticos no tienen clusters naturalmente separados (silhouette m√°ximo: 0.1759). Se forz√≥ k=4 por valor narrativo del portafolio, documentando expl√≠citamente la decisi√≥n ‚Äî exactamente lo que se har√≠a en un contexto profesional cuando el negocio requiere segmentos accionables.

**¬øPor qu√© gan√≥ Logistic Regression sobre XGBoost?**
En datasets con separaci√≥n lineal clara entre clases, LR es dif√≠cil de superar. XGBoost a√±ade complejidad que no siempre se traduce en mejora cuando el patr√≥n subyacente es relativamente simple.

---

## üìÅ Datos fuente

Los CSVs utilizados provienen de Kaggle y no se incluyen en este repositorio por tama√±o. Puedes encontrarlos buscando:

- `ecommerce customer churn dataset` ‚Äî para los archivos de clientes
- `daily ecommerce orders dataset` ‚Äî para el archivo de √≥rdenes

---
